<!-- 
    imbalance.html
    Webpage for papers within the imbalance data category
    It has a Classification and Regression section

    Author: Josias Moukpe
    Advisor: Dr. Chan
    Date: 6/14/2023

 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <title>Data Imbalanced Papers</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>


  <style>
    body {
      font-family: 'Roboto', sans-serif;
      background: linear-gradient(45deg, #8B0000, #FF4500);
      background-size: 200% 200%;
      animation: Gradient 3s ease infinite;
      color: #333;
    }

    @keyframes Gradient {
      0% {
        background-position: 0% 50%
      }

      50% {
        background-position: 100% 50%
      }

      100% {
        background-position: 0% 50%
      }
    }

    .root-container {
      margin: 2em auto;
      max-width: 90%;
      padding: 1em 2em;
      box-sizing: border-box;
      background-color: #fff;
      border-radius: 8px;
      /* box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1); */
    }

    h2 {
      color: #444;
      margin-bottom: 1em;
    }

    .paper-container {
      margin-bottom: 2em;
    }

    .paper-title {
      color: #8B0000;
    }

    a {
      color: #007BFF;
      text-decoration: none;
      font-style: italic;
      overflow-wrap: break-word;
    }

    a:hover {
      text-decoration: underline;
    }
  </style>
  <!-- <link rel="stylesheet" href="css/style.css">
        <link rel="stylesheet" href="css/imbalance.css"> -->

</head>

<body>
  <div class="root-container">
    <a href="https://cs.fit.edu/~pkc/r/readingList.html">https://cs.fit.edu/~pkc/r/readingList.html</a>
    <h2>Pertinent Machine Learning Papers</h2>
    <div class="papers-group">
      <ol>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot
              Imitation</h3>

            <p>
              <strong>BibTex</strong>
              <br>
              @inproceedings{myers2024palo, <br>
              title={Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation}, <br>
              author={Myers, Vivek and Zheng, Chunyuan and Mees, Oier and Fang, Kuan and Levine, Sergey}, <br>
              booktitle={8th Annual Conference on Robot Learning} <br>
              year={2024}} <br>

            </p>

            url=<a href="https://arxiv.org/pdf/2408.16228">https://arxiv.org/pdf/2408.16228</a><br>

            <strong>Summary</strong>
            <br>
            <p>
              The method proposed, Policy Adaptation via Language Optimization (PALO), aims to enable few-shot
              adaptation of pre-trained language-conditioned robot policies to new, unseen tasks. The key insight is to
              leverage vision-language models (VLMs) to decompose high-level instructions into smaller, more manageable
              subtasks, which the robot can execute using its existing knowledge. This approach circumvents the need for
              extensive fine-tuning on new tasks, which is often infeasible due to the high cost of collecting
              large-scale teleoperated demonstrations. Instead, PALO relies on the semantic understanding provided by
              VLMs to propose candidate decompositions of the task, using a small set of demonstrations as a calibration
              set to guide this decomposition process. The method then optimizes over these decompositions and selects
              the one that minimizes validation error on the given demonstrations.

              The core of the approach involves task decomposition through language. Given a high-level task
              instruction, the VLM generates a set of candidate decompositions that break the task into sequential
              subtasks. These subtasks are designed to align with the robot's prior training, leveraging its existing
              capabilities. Each candidate decomposition is evaluated to find the optimal one that can guide the robot
              to successfully complete the task. The evaluation process involves rolling out the pre-trained policy on
              the candidate decompositions and computing the cost based on the mean squared error between the robot's
              predicted actions and the actions demonstrated by the expert. By minimizing this error, PALO identifies
              the sequence of language subtasks that most effectively bridges the robot's prior knowledge with the new
              task requirements.

              An important aspect of PALO is the consideration of when to transition between subtasks. The method
              introduces an additional variable to represent the time steps allocated to each subtask, recognizing that
              the execution of each subtask may vary depending on the initial state and the environment's stochasticity.
              This variable is optimized alongside the task decomposition to ensure the robot executes the sequence of
              subtasks in a manner that fits the dynamics of the specific task instance. This joint optimization of
              subtasks and their timing is critical for adapting to long-horizon, multi-step tasks that require a
              precise and fluid combination of actions.

              PALO operates under two key assumptions: first, that the low-level subtasks required for the new task are
              present within the robot's prior training, and second, that the VLM can generate decompositions that are
              consistent with the semantics of the task in the given environment. These assumptions are essential for
              the method's success, as they ensure that the robot can use its pre-trained policy to execute the proposed
              subtasks effectively. By optimizing within the language space rather than directly fine-tuning policy
              parameters, PALO enables rapid adaptation with significantly fewer demonstrations, making it a
              sample-efficient approach to handling a variety of complex, unseen robotic manipulation tasks. They also
              found that
              hierarchical decomposition to subtasks as high level and low level subtasks help further improve
              performance. This paper shows that learning generalizable low level skills and knowning how to break down
              high level OoD instructions into low level InD skills helps.
            </p>

            <p>
              <strong>Problem</strong>
              Improving robot adaptability to new tasks with minimal demonstrations by leveraging vision-language models
              to semantically decompose complex instructions into executable subtasks.
            </p>
            <strong>Images</strong><br>
            <div class="row">
              <div class="col">
                <img src="../../images/ml/palo-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/ml/palo-fig2.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/ml/palo-fig3.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Concept Learning with Energy-Based Models</h3>

            <p>
              <strong>BibTex</strong>
              <br>
              @article{mordatch2018concept, <br>
              title={Concept learning with energy-based models}, <br>
              author={Mordatch, Igor}, <br>
              journal={arXiv preprint arXiv:1811.02486}, <br>
              year={2018}}<br>
            </p>

            url=<a href="https://arxiv.org/pdf/1811.02486">https://arxiv.org/pdf/1811.02486</a><br>

            <strong>Summary</strong>
            <br>
            <p>
              This paper presents a framework that uses Energy-Based Models (EBMs) to learn, identify, and generate
              abstract concepts from events in an environment. The core idea is that each concept is represented by an
              energy function, which takes a state trajectory (the sequence of entity behaviors over time), an
              attention mask (focusing on relevant entities in the event), and a concept code (parameters
              representing the concept) as inputs. The energy function returns a scalar energy value that indicates
              how well the given event configuration aligns with the specified concept. Low energy means the event
              satisfies the concept, while high energy signals a mismatch.

              The approach leverages inference-time optimization, which occurs dynamically during task execution
              rather than relying on pre-trained, fixed models. By minimizing the energy during inference, the model can
              either generate new events that fit the concept or identify existing events that match it. This method is
              more flexible than traditional learning approaches, as it adapts to new situations on the fly by adjusting
              the event parameters in real-time.

              The technique operates within a meta-learning framework, where the model generalizes its learning
              across tasks and environments. Once the energy function is trained, it can be reused in different contexts
              without retraining. This makes it particularly effective for tasks requiring few-shot learning, where
              the model needs to generalize from just a few examples. By observing a small set of demonstration events,
              the model predicts future states and identifies key entities through attention. This is formulated as a
              maximum likelihood estimation problem to maximize the probability of the observed states given the
              concept.

              The framework also employs a relational network architecture in the energy function, enabling it to
              model interactions between multiple entities, such as spatial or temporal relationships. This allows the
              model to understand and generate complex multi-entity dynamics, such as objects being "inside" or moving
              "slowly."

              To improve sampling during inference, the model uses a KL-divergence objective that refines the
              sampling process. This objective minimizes the difference between the biased sampling distribution (due to
              the truncated inference steps) and the true distribution of real-world events. This ensures that the
              generated samples during inference are realistic and align closely with the learned concepts.

              Finally, the model supports concept transfer across environments. Once a concept is learned, it can be
              transferred and applied in new environments, such as transferring concepts learned in a 2D environment to
              a 3D robot simulation. This is achieved by manually mapping representations between environments and using
              the learned energy function as a cost function in the new environment’s optimization process.

              In summary, the framework combines energy-based modeling, inference-time optimization, relational
              networks, few-shot learning, KL-divergence sampling refinement, and cross-environment concept transfer.
              These techniques enable the model to generalize from limited demonstrations, handle complex multi-entity
              concepts, and adapt to new tasks and environments, all while efficiently identifying or generating events
              that fit learned abstract concepts.
            </p>

            <p>
              <strong>Problem</strong>
              The problem being solved is learning abstract concepts from a few demonstrations and being able to
              generate or identify similar concepts in new events using energy-based models​
            </p>

            <strong>Images</strong><br>
            <div class="row">
              <div class="col">
                <img src="../../images/ml/emb-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/ml/emb-fig2.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
            <div class="row">
              <div class="col">
                <img src="../../images/ml/emb-fig3.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/ml/emb-fig4.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Learning Iterative Reasoning through Energy Diffusion</h3>

            <p>
              <strong>BibTex</strong>
              <br>
              @inproceedings{du2024ired, <br>
              title={Learning Iterative Reasoning through Energy Diffusion}, <br>
              author={Du, Yilun and Mao, Jiayuan and Tenenbaum, Joshua B}, <br>
              booktitle={Forty-first International Conference on Machine Learning} <br>
              year={2024}} <br>

            </p>

            url=<a href="https://openreview.net/pdf?id=CduFAALvGe">https://openreview.net/pdf?id=CduFAALvGe</a><br>

            <strong>Summary</strong>
            <br>
            <p>
              The paper "Learning Iterative Reasoning through Energy Diffusion" introduces IRED, a novel framework for
              solving reasoning and decision-making tasks by formulating them as energy-based optimization problems. The
              method focuses on learning an energy function E_\theta(x, y) that models the constraints between
              inputs x and outputs y, allowing the model to solve tasks via iterative optimization.

              A key innovation of IRED is its use of annealed energy landscapes. Instead of learning a single
              complex energy landscape, the model learns a sequence of smoother energy functions that progressively
              sharpen. This gradual refinement helps avoid local minima and allows for more effective optimization,
              improving the model's ability to solve complex tasks. The energy minimization process is adaptive, meaning
              the number of optimization steps during inference is adjusted based on the problem's difficulty. This
              enables IRED to solve tasks beyond its training distribution, such as more challenging Sudoku puzzles,
              large matrix completion problems, and complex pathfinding tasks.

              The training process incorporates two key supervision techniques: score function supervision and
              energy landscape supervision. Score function supervision addresses instability in energy-based
              model training by directly optimizing the gradients of the energy function, resulting in more stable and
              faster training. Energy landscape supervision ensures that the energy function can effectively
              distinguish between correct and incorrect outputs by using negative sample mining, which forces the model
              to assign higher energy to invalid outputs, sharpening the energy landscape.

              IRED’s model architecture consists of a neural network that learns the energy function. During inference,
              the model performs energy minimization over the learned energy function to find
              the optimal output. This process is iterative, where the model uses gradient-based optimization to
              minimize the energy
              landscape progressively. The key feature here is that this iterative optimization is done on a sequence of
              annealed energy landscapes, making the search for the optimal solution more effective and efficient.

              IRED outperforms traditional methods in reasoning tasks across continuous and discrete spaces, as well as
              planning problems. It generalizes well to more difficult instances of these tasks by dynamically adjusting
              its computational effort during inference, making it a robust and versatile framework for a wide range of
              reasoning problems.
            </p>

            <p>
              <strong>Problem</strong>
              The main problem solved is developing
              a framework that learns to reason across different tasks by using energy-based optimization, enabling
              generalization to more complex problems beyond its training distribution​
            </p>

            <strong>Images</strong><br>
            <div class="row">
              <div class="col">
                <img src="../../images/ml/ired-fig1.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Attention is not all you need: Pure attention loses rank doubly exponentially with
              depth</h3>

            <p>
              <strong>BibTex</strong>
              <br>
              @inproceedings{dong2021attentionNotAllYouNeed, <br>
              title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth}, <br>
              author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas}, <br>
              booktitle={International Conference on Machine Learning}, <br>
              year={2021}} <br>
            </p>

            url=<a
              href="https://proceedings.mlr.press/v139/dong21a/dong21a.pdf">https://proceedings.mlr.press/v139/dong21a/dong21a.pdf</a><br>

            <strong>Summary</strong>
            <br>
            <p>

              The authors introduce a novel way to analyze self-attention networks (SANs) through what they call "path
              decomposition." This approach breaks down a SAN's output into a sum of smaller terms, where each term
              represents a path through different attention heads across layers. Think of attention heads as gateways,
              and a path is like choosing one gateway per layer to pass through. This decomposition allows them to study
              how information flows through the network and how the rank collapse phenomenon occurs.

              The paper presents a rigorous mathematical analysis of how pure attention networks (without skip
              connections or MLPs) process information. They prove that these networks have a strong inductive bias
              toward making all tokens uniform, which manifests as a doubly exponential convergence to a rank-1 matrix
              with network depth. A rank-1 matrix means all output rows are identical, effectively destroying the
              network's ability to produce diverse outputs for different inputs.

              The authors then systematically analyze how different architectural components affect this rank collapse.
              They demonstrate that skip connections play a crucial role in preventing rank collapse by providing paths
              that preserve input information. MLPs also help counteract rank collapse by potentially increasing the
              rank through their non-linear transformations, though their effect is less dramatic than skip connections.
              Interestingly, they prove that layer normalization, another common component in transformers, plays no
              role in preventing rank collapse.

              Their methodology involves developing new mathematical tools to analyze the convergence rates of these
              networks. They introduce concepts like the `1,∞-composite norm and derive bounds on how quickly the
              residual (the difference between the output and its rank-1 approximation) decreases with network depth.
              The analysis carefully tracks how the interaction between stochastic attention matrices and value
              transformations affects the network's output rank.

              The authors also develop a theoretical framework to understand why transformers work in practice despite
              the potential for rank collapse. They show that transformer networks effectively operate as ensembles of
              shallow networks due to skip connections, with shorter paths carrying more predictive power than longer
              ones. This insight provides a new perspective on transformer architecture design and suggests ways to make
              these networks more effective.
            </p>

            <p>
              <strong>Problem</strong>
              The paper discusses how pure self-attention networks (without skip connections or MLPs) suffer from "rank
              collapse" - their output converges doubly exponentially to a rank-1 matrix as network depth increases,
              meaning all output vectors become identical copies of each other, severely limiting the network's ability
              to produce diverse outputs.
            </p>

            <strong>Images</strong><br>
            <div class="row">
              <div class="col">
                <img src="../../images/repr/attentionNotAllYouNeed-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/repr/attentionNotAllYouNeed-fig2.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">π0: A Vision-Language-Action Flow Model for General Robot Control</h3>

            <p>
              <strong>BibTex</strong>
              <br>
              @inproceedings{black2024pi0, <br>
              title={π0: A Vision-Language-Action Flow Model for General Robot Control}, <br>
              author={Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn,
              Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and
              Jones, Tim and Ke, Liyiming and Levine, Sergey and Li-Bell, Adrian and Mothukuri, Mohith and Nair, Suraj
              and Pertsch, Karl and Shi, Lucy Xiaoyang and Tanner, James and Vuong, Quan and Walling, Anna and Wang,
              Haohuan and Zhilinsky, Ury}, <br>
              booktitle={Physical Intelligence Company}, <br>
              year={2024}} <br>
            </p>

            url=<a href="https://www.physicalintelligence.company/download/pi0.pdf">
              https://www.physicalintelligence.company/download/pi0.pdf</a><br>

            <strong>Summary</strong>
            <br>
            <p>

              The authors present π0, a vision-language-action (VLA) Flow Matching model designed to make robots more
              versatile and capable of handling diverse manipulation tasks. The model takes multiple RGB camera images
              (2-3 per robot), language commands, and the robot's joint state as input, and outputs action sequences
              that control the robot at 50Hz (one action every 20ms). These actions can control complex robot
              configurations with up to 18 dimensions to handle multiple arms and mobile bases. The key innovation lies
              in their training approach, which mirrors the successful paradigm used in large language models:
              pre-training on diverse, large-scale data followed by task-specific fine-tuning. This approach helps
              address fundamental challenges in robot learning: data scarcity, limited generalization, and lack of
              robustness.

              The model architecture builds upon the PaliGemma vision-language model and incorporates flow matching to
              generate continuous robot actions. A crucial architectural innovation is the "action expert" - a separate
              set of weights dedicated to handling robotics-specific inputs and outputs. This design allows the model to
              leverage pre-trained vision-language knowledge while maintaining precise control over robot movements. The
              model processes multiple camera views, language commands, and robot state information to generate
              high-frequency action sequences for dexterous manipulation.

              The training process involves two key phases. In pre-training, they use over 10,000 hours of robot data
              from 7 different robot configurations and 68 diverse tasks, combined with existing open-source datasets.
              This creates a broad foundation of manipulation skills. The post-training phase then uses high-quality,
              task-specific data to refine the model for particular applications. This two-phase approach is crucial:
              the diverse pre-training data helps the model learn to recover from mistakes and handle varied situations,
              while the post-training data teaches it to execute tasks efficiently and fluently.

              A major strength of their approach is its ability to handle complex, multi-stage tasks. The model can be
              guided either through direct language commands from humans or through a high-level policy that breaks down
              complex tasks into simpler subtasks. This enables the system to perform sophisticated behaviors like
              folding multiple articles of clothing, cleaning tables while appropriately sorting items between trash and
              dishes, and assembling boxes - tasks that require both physical dexterity and strategic planning.
            </p>

            <p>
              <strong>Problem</strong>
              Robot learning currently faces three major challenges: data scarcity which limits training, limited
              generalization across different manipulation tasks and robots, and lack of robustness to handle unexpected
              situations and environmental variations in real-world settings.
            </p>

            <strong>Images</strong><br>
            <div class="row">
              <div class="col">
                <img src="../../images/repr/pizero-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/repr/pizero-fig2.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">LoRA vs Full Fine-tuning: An Illusion of Equivalence</h3>

            <p>
              <strong>BibTex</strong>
              <br>
              @article{shuttleworth2024loravft, <br>
              title={LoRA vs Full Fine-tuning: An Illusion of Equivalence}, <br>
              author={Shuttleworth, Reece and Andreas, Jacob and Torralba, Antonio and Sharma, Pratyusha}, <br>
              journal={arXiv preprint arXiv:2410.21228}, <br>
              year={2024}} <br>
            </p>

            url=<a href="https://arxiv.org/pdf/2410.21228">https://arxiv.org/pdf/2410.21228</a><br>

            <strong>Summary</strong>
            <br>
            <p>
              The paper "LoRA vs Full Fine-Tuning: An Illusion of Equivalence" examines the structural and behavioral
              differences between Low-Rank Adaptation (LoRA) and full fine-tuning of pre-trained language models.
              Although LoRA matches full fine-tuning performance with fewer trainable parameters, the paper reveals that
              these methods produce fundamentally different solutions.

              The authors find that LoRA introduces "intruder dimensions"—high-ranking singular vectors orthogonal to
              pre-trained vectors—which do not appear in full fine-tuning. This structural divergence impacts model
              behavior, causing LoRA models to forget more of the pre-training distribution and adapt less robustly in
              continual learning scenarios. Despite similar performance on target tasks, LoRA-fine-tuned models exhibit
              reduced generalization due to these intruder dimensions. Full fine-tuning, on the other hand, preserves
              the alignment with pre-trained singular vectors, maintaining spectral similarity and higher effective rank
              in weight matrices.

              The study also highlights that even when LoRA uses a high-rank configuration, its effective rank remains
              lower than that of full fine-tuning. This limitation points to LoRA's underutilization of parameter space,
              explaining its struggles with complex tasks. Furthermore, an experiment isolating matrix <i>B</i> in
              LoRA—where only <i>B</i> is trained while <i>A</i> remains fixed—shows a sharp reduction in intruder
              dimensions. This suggests that the matrix product <i>BA</i> amplifies spectral differences, contributing
              to
              the issue.

              The authors conclude that while LoRA is computationally efficient, its distinct parameter updates lead to
              reduced generalization unless higher ranks or stabilizing strategies are employed. This insight emphasizes
              the trade-offs between LoRA’s parameter efficiency and the robustness of full fine-tuning.
            </p>

            <p>
              <strong>Problem</strong>
              The paper analyzes whether fine-tuning pre-trained language models using LoRA yields solutions equivalent
              to those obtained through full fine-tuning, particularly by examining their structural differences,
              generalization behaviors, and spectral properties.
            </p>

            <strong>Images</strong><br>
            <div class="row">
              <div class="col">
                <img src="../../images/repr/loravft-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/repr/loravft-fig2.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
            <div class="row">
              <div class="col">
                <img src="../../images/repr/loravft-fig3.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/repr/loravft-fig4.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Differential Transformer</h3>

            <p>
              <strong>BibTex</strong>
              <br>
              @article{ye2024diffT, <br>
              title={Differential Transformer}, <br>
              author={Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
              <br>
              journal={arXiv preprint arXiv:2410.05258}, <br>
              year={2024}} <br>
            </p>

            url=<a href="https://arxiv.org/pdf/2410.05258">
              https://arxiv.org/pdf/2410.05258
            </a><br>

            <strong>Summary</strong>
            <br>
            <p>
              The Differential Transformer (DIFF Transformer) is a novel enhancement to the standard Transformer
              architecture designed to improve the efficiency and accuracy of attention mechanisms in large language
              models. Traditional Transformers often struggle with attention allocation, frequently focusing on
              irrelevant parts of the input context, a phenomenon referred to as attention noise. This misallocation can
              hinder their performance in tasks that require precise retrieval of key information, especially in
              long-context scenarios.

              The DIFF Transformer introduces a unique differential attention mechanism that calculates attention scores
              by subtracting one softmax attention map from another. This mechanism starts by dividing the query and key
              vectors into two groups, producing separate softmax attention maps for each group. The attention scores
              are then derived from the difference between these maps, effectively canceling out shared noise. This
              process enhances the model's focus on relevant context while minimizing distractions, similar to how
              noise-canceling technology works by subtracting ambient noise from a signal. The resulting attention
              pattern is sparser and more precise, which helps the model prioritize crucial information.

              This differential attention mechanism can be seamlessly integrated with the existing multi-head attention
              framework, where each head independently performs this subtraction-based attention (delta). The learnable
              scalar λ
              controls the degree of noise cancellation and adapts throughout training to optimize performance. This
              method leads to more efficient attention operations without significantly increasing computational
              complexity.

              Experimentally, the DIFF Transformer has demonstrated superior performance over traditional Transformers
              across a variety of tasks, including language modeling and key information retrieval. It also shows
              reduced attention noise and improved focus, making it robust for long-sequence processing and complex
              scenarios.
            </p>

            <p>
              <strong>Problem</strong>
              The Differential Transformer aims to address the issue of excessive attention to irrelevant context in
              standard Transformers by introducing a mechanism that enhances focus on relevant information while
              reducing attention noise.
            </p>

            <strong>Images</strong><br>
            <div class="row">
              <div class="col">
                <img src="../../images/repr/difft-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/repr/difft-fig2.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
            <div class="row">
              <div class="col">
                <img src="../../images/repr/difft-fig3.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/repr/difft-fig4.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">nGPT: Normalized Transformer with Representation Learning on the Hypersphere</h3>

            <p>
              <strong>BibTex</strong>
              <br>
              @article{loshchilov2024ngpt, <br>
              title={ngpt: Normalized transformer with representation learning on the hypersphere}, <br>
              author={Loshchilov, Ilya and Hsieh, Cheng-Ping and Sun, Simeng and Ginsburg, Boris}, <br>
              journal={arXiv preprint arXiv:2410.01131}, <br>
              year={2024}} <br>
            </p>

            url=<a href="https://arxiv.org/pdf/2410.01131">
              https://arxiv.org/pdf/2410.01131
            </a><br>

            <strong>Summary</strong>
            <br>
            <p>
              The nGPT method enhances the standard Transformer by incorporating a normalization strategy that
              constrains all vectors involved—such as embeddings, MLP, attention matrices, and hidden states—to have
              unit norm on a hypersphere. This normalization ensures that matrix-vector multiplications can be
              interpreted as cosine similarities, bounded between -1 and 1, which eliminates the need for weight decay
              and stabilizes training.

              In nGPT, each input token is represented on the surface of a hypersphere, and the model iteratively
              adjusts its position with updates determined by attention and MLP layers. These updates are controlled by
              trainable eigen learning rates, represented as diagonal elements in a variable-metric matrix. The updates
              are followed by normalization steps, which act as retraction steps in Riemannian optimization, keeping the
              vectors on the hypersphere and maintaining consistency across layers.

              The attention mechanism in nGPT applies normalization to the query and key vectors after projecting them,
              ensuring that their dot products represent controlled, bounded similarities. This adjustment also modifies
              the softmax scaling to restore variance, improving the model's stability. In the MLP block, similar
              normalization is applied, with scaling factors introduced to maintain the impact of non-linearities like
              the SiLU activation.

              The method has demonstrated significant efficiency gains, requiring 4 to 20 times fewer training steps to
              achieve the same accuracy as conventional Transformers, depending on context length. This approach
              simplifies training by removing the need for additional normalization layers and weight decay, thus
              optimizing performance for large-scale language models.
            </p>

            <p>
              <strong>Problem</strong>
              The nGPT model addresses the problem of improving training stability and efficiency in Transformers by
              normalizing all vectors (embeddings, attention matrices, etc.) on a hypersphere, leading to faster
              convergence and better representation learning​
            </p>

            <strong>Images</strong><br>
            <div class="row">
              <div class="col">
                <img src="../../images/repr/ngpt-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/repr/ngpt-fig2.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                <img src="../../images/repr/ngpt-fig3.png" class="img-fluid rounded" alt="...">
              </div>
            </div>
          </div>
        </li>

        <li>
          <div class="paper-container">
            <h3 class="paper-title">Why Should I Trust You?" Explaining the Predictions of Any Classifier</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
               <div class="col">
                   <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
               </div>
               <div class="col">
                   <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
               </div>
           </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">A Unified Approach to Interpreting Model Predictions</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
               <div class="col">
                   <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
               </div>
               <div class="col">
                   <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
               </div>
           </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Axiomatic attribution for deep networks</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
               <div class="col">
                   <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
               </div>
               <div class="col">
                   <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
               </div>
           </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Towards better understanding of gradient-based attribution methods for Deep Neural
              Networks </h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
               <div class="col">
                   <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
               </div>
               <div class="col">
                   <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
               </div>
           </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation
              Methods</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
               <div class="col">
                   <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
               </div>
               <div class="col">
                   <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
               </div>
           </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Do Feature Attribution Methods Correctly Attribute Features?</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
               <div class="col">
                   <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
               </div>
               <div class="col">
                   <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
               </div>
           </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">mixup: Beyond Empirical Risk Minimization</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
               <div class="col">
                   <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
               </div>
               <div class="col">
                   <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
               </div>
           </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Manifold Mixup: Better Representations by Interpolating Hidden States</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
               <div class="col">
                   <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
               </div>
               <div class="col">
                   <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
               </div>
           </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Revisiting Deep Learning Models for Tabular Data</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
              <div class="col">
                  <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                  <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
              </div>
          </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in
              Deep
              Learning</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
              <div class="col">
                  <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                  <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
              </div>
          </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Robustness via cross-domain ensembles</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
              <div class="col">
                  <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                  <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
              </div>
          </div> -->
          </div>
        </li>
        <li>
          <div class="paper-container">
            <h3 class="paper-title">Online Knowledge Distillation via Collaborative Learning</h3>

            <p>
              <strong>BibTex</strong>
              <br>

            </p>

            url=<a href=""></a><br>

            <strong>Summary</strong>
            <br>
            <p>

            </p>

            <p>
              <strong>Problem</strong>

            </p>

            <strong>Images</strong><br>
            <!-- <div class="row">
              <div class="col">
                  <img src="../../images/ml/saint-fig1.png" class="img-fluid rounded" alt="...">
              </div>
              <div class="col">
                  <img src="../../images/ml/saint-fig2.png" class="img-fluid rounded" alt="...">
              </div>
          </div> -->
          </div>
        </li>
      </ol>
    </div>
  </div>
</body>

</html>