<!-- 
    imbalance.html
    Webpage for papers within the imbalance data category
    It has a Classification and Regression section

    Author: Josias Moukpe
    Advisor: Dr. Chan
    Date: 6/14/2023

 -->
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Data Imbalanced Papers</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>



    <style>
        body {
            font-family: 'Roboto', sans-serif;
            background: linear-gradient(45deg, #8B0000, #FF4500);
            background-size: 200% 200%;
            animation: Gradient 3s ease infinite;
            color: #333;
        }

        @keyframes Gradient {
            0% {
                background-position: 0% 50%
            }

            50% {
                background-position: 100% 50%
            }

            100% {
                background-position: 0% 50%
            }
        }

        .root-container {
            margin: 2em auto;
            max-width: 90%;
            padding: 1em 2em;
            box-sizing: border-box;
            background-color: #fff;
            border-radius: 8px;
            /* box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1); */
        }

        h2 {
            color: #444;
            margin-bottom: 1em;
        }

        .paper-container {
            margin-bottom: 2em;
        }

        .paper-title {
            color: #8B0000;
        }

        a {
            color: #007BFF;
            text-decoration: none;
            font-style: italic;
        }

        a:hover {
            text-decoration: underline;
        }
    </style>
    <!-- <link rel="stylesheet" href="css/style.css">
        <link rel="stylesheet" href="css/imbalance.css"> -->

</head>

<body>
    <div class="root-container">
        <a href="https://cs.fit.edu/~pkc/r/readingList.html">https://cs.fit.edu/~pkc/r/readingList.html</a>
        <h2>Classification</h2>
        <div class="papers-group">
            <ol>
                <li>
                    <div class="paper-container">
                        <h3 class="paper-title"> Decoupling Representation and Classifier for Long-Tailed
                            Recognition</h3>
                        <p>
                            <strong>BibTex</strong>
                            <br>
                            @inproceedings{
                            kang2020decoupling, <br>
                            title={Decoupling Representation and Classifier for Long-Tailed Recognition}, <br>
                            author={Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo,
                            Albert and Feng, Jiashi and Kalantidis, Yannis}, <br>
                            booktitle={International Conference on Learning Representations}, <br>
                            year={2020}}
                        </p>
                        url=<a
                            href="https://iclr.cc/virtual_2020/poster_r1gRTCVFvB.html">https://iclr.cc/virtual_2020/poster_r1gRTCVFvB.html</a><br>
                        <strong>Summary</strong>
                        <br>
                        <p>
                            In this paper, the authors are tackling the problem of classification on long-tail datasets.
                            The authors
                            propose decoupling representation learning and classifier learning for long-tail data
                            distributions unlike previous
                            methods which would train a model jointly for both representation and classifier learning.
                            One advantage of decoupling
                            that the autors point out is the ability to use different sampling techniques for the stages
                            and see which sample technique
                            benefit best each stage. To that end, The authors setup a Convolutional Neural Network which
                            is composed of a backbone for
                            representation learning and a classifier head, either linear model or a MultiLayer
                            Perceptron, for classification. They then
                            employ various sampling techniques for both parts independently. For learning
                            representations, the sampling techniques are <br>
                        <ul>
                            <li>Instance balanced sampling: each training example has equal probability of being
                                sampled. As a results classes with
                                greater number of samples dominate the training process. </li>
                </li>
                <li>Class-balanced sampling: it occurs in two steps where a class is uniformly selected from the set of
                    all classes and
                    then the sample is uniformly selected from the selected class. This sampling technique is used to
                    balance the number
                    of samples per class. </li>
                </li>
                <li>Square-root sampling: where the Square-root of the number of examples is considered instead of the
                    number of examples
                    itself.</li>
                </li>
                <li>Progressively balanced sampling: it's a mix of Instance balanced sampling and Class balanced
                    sampling. Using the
                    iteration number (epoch number), the sampling can start with Instance balanced sampling and then
                    gradually shift to
                    Class balanced sampling. Unfortunately, this technique requires the total number of iterations to be
                    known ahead of
                    time.
                </li>
                </ul>
                <br>
                For classifier learning, the approaches are <br>
                <ul>
                    <li>cRT: Classifier Re-training where the classifier head is retrained on class-balanced sampling
                    </li>
                    <li>Nearest Class Mean Cluster: where a new example is classified based on its proximity to the
                        nearest class cluster
                        mean. This technique rely more heavily on the quality of the representation learning. </li>
                    <li> tau normalized classifier: where the norms of the classifier weights associated with each class
                        are normalized to
                        to prevent the majority classes' weight to grow dominant as they normally would </li>
                    </li>
                    <li>LWS: Learnable Weights Scaling takes that step further and makes to normalization coefficient of
                        the class associated
                        weights learnable
                    </li>
                </ul><br>

                After the test on few long-tail datasets, the authors show that data imbalance is not an issues for
                learning high quality representations.
                Moreover, representation learning is not affected by tail distribution but rather takes advantage
                of the instance-balanced distribution. But classifier learning is affected by the tail
                distribution and therefore needs to be balanced. The best coupling the found was instance-balanced
                sampling for representation learning
                and some balancing for classifier learning (cRT, tau-normalized, LWS).

                </p>
                <p>
                    <strong>Problem</strong>
                    imbalanced input distribution cause the model to be biased towards the majority classes and
                    therefore perform poorly on the minority classes.
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    The authors main approach is to decouple the representation learning and
                    classifier learning. That way, a different sampling technique can be applied to
                    the representation learning and the classifier learning. The authors show that
                    representation learning is not affected by tail distribution and therefore can
                    leverage all data points. It's actually best for the representation learning to
                    use instance-balanced sampling. The classifier can then be adapted to the imbalance
                    of the data through few method that the author compared. The ones that work the best
                    are cRT (classifier retraining where the classifier is retrained on a class-balanced
                    sampling), thau-normalized (where the classifier's weights are normalized inversely
                    proportional to the class frequency), and LWS (learnable weight scaling where the
                    thau normalized factor is learned).

                    </ul>
                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/decoupling-fig1.png" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/decoupling-fig2.png" class="img-fluid rounded" alt="...">
                    </div>

                </div>
        </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title"> BBN: Bilateral-Branch Network with Cumulative Learning
                    for Long-Tailed Visual Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    zhou2020bbn, <br>
                    title={Bbn: Bilateral-branch network with cumulative learning for long-tailed visual
                    recognition}, <br>
                    author={Zhou, Boyan and Cui, Quan and Wei, Xiu-Shen and Chen, Zhao-Min}, <br>
                    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern
                    recognition}, <br>
                    year={2020}}
                </p>
                url=<a
                    href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_BBN_Bilateral-Branch_Network_With_Cumulative_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2020_paper.pdf">
                    https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_BBN_Bilateral-Branch_Network_With_Cumulative_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2020_paper.pdf</a><br>
                <strong>Summary</strong>
                <br>
                <p>
                    In this paper, the authors are tackling the problem of classification on long-tail data
                    distributions.
                    They observed that balancing methods (when performed jointly) promote minority classes at the
                    expense of the majority classes.
                    They show through their experiments that for representation learning, using the original long-tail
                    distribution has the
                    least error rate overall, while for classifier learning, using a balanced distribution has the least
                    error rate overall. In an effort
                    to reconcile the two, they propose a bilateral-branch network (BBN) that has two branches: one
                    branch called the convential branch
                    that cares about learning the original distribution for strong representation learning, and another
                    branch called the re-balancing branch
                    that cares about learning a reverse distribution (minority classes are more likely to be sampled)
                    for strong classifier learning. It's crucial to note
                    that both branches share the same weights for the representation learning backbone. Both branches
                    would then be combined through a cumulative learning strategy. The cumulative learning strategy
                    features two classifier from both branches. A classifier from the conventional branch to classify
                    majority classes and a classifier from the
                    re-balancing branch to classify minority classes. The cumulative learning strategy is a weighted sum
                    of the two branches' outputs.
                    That weight is modulated by the iteration number and follows a parabolic decay. Early in the
                    training, more weight is given to the conventional branch
                    and much later in the training, more weight is given to the re-balancing branch. The authors also
                    propose a weighted cross-entropy loss that accompany
                    the cumulative learning strategy. During inference, both branches are considered equally and the
                    final prediction is the average of the two branches' predictions.
                    Their results show that their soft decoupling by transitioning from conventional learning to
                    re-balancing learning is better than technique prior. The
                    norms of the classifier weights are also shown to be more balanced, with a lower standard deviation,
                    than the prior techniques.
                </p>

                <p>
                    <strong>Problem</strong>
                    imbalanced input distribution cause the classifier to be biased towards majority classes even though
                    the feature extractor is
                    performant under the original distribution.
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong><br>
                    2 branches are used: one branch for representation learning and another branch for classifier
                    learning. The representation learning branch
                    is trained on the original distribution while the classifier learning branch is trained on a reverse
                    distribution. The two branches are then combined
                    through a cumulative learning strategy where the weight of each branch is modulated by the iteration
                    number according to a curriculum.
                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/bbn-fig1.png" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/bbn-fig2.png" class="img-fluid rounded" alt="...">
                    </div>

                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Balanced Meta-Softmax for Long-Tailed Visual Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    ren2020balsoftmax, <br>
                    title={Balanced Meta-Softmax for Long-Tailed Visual Recognition}, <br>
                    author={Ren, Jiawei and Yu, Cunjun and sheng, shunan and Ma, Xiao and Zhao, Haiyu and Yi, Shuai and
                    Li, hongsheng}, <br>
                    booktitle={Advances in Neural Information Processing Systems}, <br>
                    year={2020}}
                </p>

                url=<a
                    href="https://proceedings.neurips.cc/paper_files/paper/2020/file/2ba61cc3a8f44143e1f2f13b2b729ab3-Paper.pdf">
                    https://proceedings.neurips.cc/paper_files/paper/2020/file/2ba61cc3a8f44143e1f2f13b2b729ab3-Paper.pdf</a><br>
                <strong>Summary</strong>
                <br>
                <p>
                    The authors are tackling the problem of classification on long-tail data distributions. They
                    observed that
                    the softmax function is not well suited for long-tail data distributions as it gives bias gradient
                    estimates
                    under long-tail data distributions. They therefore propose a balanced softmax function that is only
                    used during training
                    while the conventional softmax function is used during testing or inference (in deployment).
                    Utilizing the bayesian definition of the softmax function,
                    they propose a balanced softmax function that is softmax with the number of samples in each class a
                    coefficient to the exponential. This formulation
                    of a balanced softmax during training forces the training network to output larger logits for
                    minority classes. However, paired with a class balanced
                    strategy, the authors observed that minority classes were overbalanced. They needed a new sampling
                    strategy that would work with the balanced softmax
                    function. The proposed a meta sampler, a model that could learn that optimal sampling strategy for
                    the balanced softmax function during training.
                    The training routine would be in two level where the first level would be the training of the meta
                    sampler in the inner training loop and then
                    the training of the network in the outer training loop. The algorithmic steps are as follows:
                <ul>
                    <li>
                        Obtain a minibach sampled with the meta sampler from the training set and use that minibach to
                        train a surrogate network.
                        The surrogate network is used there to ensure the real network can be trained with a better
                        batch from the meta sampler later.
                        This step is done once per outer loop iteration
                    </li>
                    <li>
                        A loss for the surrogate network is computed on a class balanced dataset with softmax and cross
                        entropy and the value of the loss
                        was used to update the meta sampler. This update is performed a few times per outer loop
                        iteration.
                    </li>
                    <li>
                        The improved sampled is then used to sample a new batch for the real network to train on. This
                        step is done once per outer loop iteration.
                    </li>
                </ul>
                To allow end to end training, the authors used the reparameterization trick to allow the gradient to
                flow through the meta sampler.
                The authors showed that their method outperformed the state-of-the-art methods and could be paired with
                decouple learning methods to further improve performance. Their method shows strong balance between the
                minority and majority classes
                on benchmark Long Tail datasets.

                </p>

                <p>
                    <strong>Problem</strong>
                    output logits are not balanced for long-tail data distributions, majority classes have larger logits
                    than minority classes. This may
                    lead to poor performance on minority classes.
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    The authors propose a balanced softmax function that is only used during training. The balanced
                    softmax function is softmax with the number of samples in each
                    class a coefficient to the exponential. This formulation of a balanced softmax during training
                    forces the training network to output larger logits for minority classes.
                    However, paired with a class balanced strategy, the authors observed that minority classes were
                    overbalanced.
                    A meta sampler is proposed to learn the optimal sampling strategy for the balanced softmax function
                    during training. The training routine would be in two level where the first level would be the
                    training of the meta sampler in the inner training loop and then
                    the training of the network in the outer training loop.

                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/meta-softmax-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/meta-softmax-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Exploring balanced feature spaces for representation learning</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    kang2021kcl, <br>
                    title={Exploring balanced feature spaces for representation learning}, <br>
                    author={Kang, Bingyi and Li, Yu and Xie, Sa and Yuan, Zehuan and Feng, Jiashi}, <br>
                    booktitle={International Conference on Learning Representations}, <br>
                    year={2021}}
                </p>
                url=<a
                    href="https://openreview.net/pdf?id=OqtLIabPTit">https://openreview.net/pdf?id=OqtLIabPTit</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    The authors are tackling the issue of imbalance feature space for long-tail data distributions. They
                    observed that the feature space is imbalanced
                    under long-tail data distributions which leads to poor decision boundaries for minority classes when
                    using Supervised Cross Entropy loss. They
                    observed that with contrastive loss, the feature space is balanced. They therefore propose a
                    supervised contrastive learning method that uses
                    the labels of the data to learn a representation space where similar samples are close to each other
                    and dissimilar samples are far from each other.
                    The supervised contrastive loss use class label to draw samples from the same class close to each
                    other and samples from different classes far from each other.
                    The noticed however that, with a large number of samples, the majority classes would dominate the
                    learning process. they then proposed
                    K contrastive learning (KCL) that limits the number of positive samples from each class to K to
                    prevent the majority classes from dominating the learning process with more samples.
                    They showed their methods also works on other tasks such as object detection and semantic
                    segmentation. a proposed a balancedness metric to measure the balance of the feature space.
                </p>

                <p>
                    <strong>Problem</strong>
                    under long tail distribution, the feature space is imbalanced which leads to poor decision
                    boundaries for minority classes.
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    supervised contrastive learning to learn a balanced feature space. Supervised contrastive learning
                    is a contrastive learning method that uses
                    the labels of the data to learn a representation space where similar samples are close to each other
                    and dissimilar samples are far from each other.
                    K contrastive learning (KCL) is a supervised contrastive learning method that the limit of the
                    number of samples from each class to K to prevent
                    the majority classes from dominating the learning process with more samples.

                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/kcl-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/kcl-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Long-tail learning via logit adjustment</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    menon2021logitadjustment, <br>
                    title={Long-tail learning via logit adjustment}, <br>
                    author={Menon, Aditya Krishna and Jayasumana, Sadeep and Rawat, Ankit Singh and Jain, Himanshu and
                    Veit, Andreas and Kumar, Sanjiv}, <br>
                    booktitle={International Conference on Learning Representations}, <br>
                    year={2021}}
                </p>
                url=<a href="https://openreview.net/pdf?id=37nvvqkCo5">https://openreview.net/pdf?id=37nvvqkCo5</a><br>
                <strong>Summary</strong>
                <br>
                <p>
                    The authors propose a framework to generalize on the logit adjustment methods. They observed that
                    previous methods focused on scaling the logits and assumed the target distribution is uniform.
                    They propose a logit adjustment framework that can be implemented post hoc to any existing model
                    where an adjustment term based on the prior distribution of the target dataset is added to the
                    logits.
                    It can also be implemented as a loss function. The loss function would be adjusted based on added
                    terms that would take into account the known prior of the
                    target dataset. The authors showed that their method indeed was a general formulation of previous
                    methods and that it outperformed previous methods on benchmark datasets.
                </p>

                <p>
                    <strong>Problem</strong>
                    logits are not balanced for long-tail data distributions, majority classes have larger logits than
                    minority classes. furthermore, previous
                    attempts at adjusting logits focus on scaling the logits and assumed the target distribution is
                    uniform
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    a logit adjustment framework that can be implemented post hoc to any existing model where an
                    adjustment term based on the prior distribution
                    of the target dataset is added to the logits. It can also be implemented as a loss function.


                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/logitadj-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/logitadj-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Long-tailed Recognition by Routing Diverse Distribution-Aware Experts</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    wang2020ride, <br>
                    title={Long-tailed Recognition by Routing Diverse Distribution-Aware Experts}, <br>
                    author={Wang, Xudong and Lian, Long and Miao, Zhongqi and Liu, Ziwei and Yu, Stella}, <br>
                    booktitle={International Conference on Learning Representations}, <br>
                    year={2020}}
                </p>
                url=<a
                    href="https://openreview.net/pdf?id=D9I3drBz4UC">https://openreview.net/pdf?id=D9I3drBz4UC</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    This paper demonstrate that prior methods that tackle the long-tail problem by focusing on the
                    balacing the classifier
                    decrease tail bias at the expense of increased head bias, and increased variance for all classes.
                    They tackle this issue
                    with an ensemble of experts that are trained on balanced data and to be diverse ie their posteriors
                    distribution are divergent. To make sure
                    not too many experts are used which would take too much compute, they use a router that predicts
                    whether to involve additional experts in the classification.
                    The router network is trained to route to the next expert if the current expert incorrectly classify
                    but some kth expert correctly classify the sample.
                    They optionally apply self distillation to distill many experts into fewer experts.
                </p>

                <p>
                    <strong>Problem</strong>
                    imbalance classification and focusing reduce tail bias at expense of increased head bias, increase
                    variance for all classes
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    use multiple experts to reduce variance, and add loss term to increase experts diversity and reduce
                    bias.
                    learne router to predict whether to involve additional experts in the classification

                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/ride-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/ride-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification
                </h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    wang2021hybridsc, <br>
                    title={Contrastive learning based hybrid networks for long-tailed image classification}, <br>
                    author={Wang, Peng and Han, Kai and Wei, Xiu-Shen and Zhang, Lei and Wang, Lei}, <br>
                    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, <br>
                    year={2021}}
                </p>

                url=<a
                    href=https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Contrastive_Learning_Based_Hybrid_Networks_for_Long-Tailed_Image_Classification_CVPR_2021_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Contrastive_Learning_Based_Hybrid_Networks_for_Long-Tailed_Image_Classification_CVPR_2021_paper.pdf</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    This paper tackle imbalance classification and the memory inefficiency observed in current
                    supervised contrastive learning methods where they require large amount
                    of negative samples. To tackle this issues, they propse a 2 branch network with a shared backbone.
                    One branch is trained on imbalanced data samples with an MLP
                    to produce embedding based on supervised contrastive loss that leverages class prototypes as
                    positive and negative samples. The other branch is trained on balanced data samples
                    with a linear layer to produce good decision boundaries for the features based on supervised cross
                    entropy loss. The 2 branches are trained in a curriculum fashion with alpha
                    controlling the balance between the 2 branches. Alpha decays as linearly with the epochs, indicating
                    the training starts first with the feature learning branch and then
                    gradually shift to the classifier learning branch. More than one prototype per class can be used to
                    improve the performance of the feature learning branch (still fewer in number
                    compared to negative samples needed). The prototypes are learned.

                </p>

                <p>
                    <strong>Problem</strong>
                    memory ineffficient feature learning
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    use class prototypes in supervised contrastive loss.
                    2 branches and curriculum training from one to the other


                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/hybridsc-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/hybridsc-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Disentangling Label Distribution for Long-tailed Visual Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    hong2021disentangling, <br>
                    title={Disentangling label distribution for long-tailed visual recognition}, <br>
                    author={Hong, Youngkyu and Han, Seungju and Choi, Kwanghee and Seo, Seokjun and Kim, Beomsu and
                    Chang, Buru}, <br>
                    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, <br>
                    year={2021}}
                </p>
                url=<a
                    href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Disentangling_Label_Distribution_for_Long-Tailed_Visual_Recognition_CVPR_2021_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Disentangling_Label_Distribution_for_Long-Tailed_Visual_Recognition_CVPR_2021_paper.pdf</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    To tackle imbalance classification, the authors notice that previous methods focus on assumed
                    uniform target distribution. They propose to not assume the
                    target distribution is uniform but still known. Based on that, they reframe the problem as a
                    distribution shift problem and disentangle the source distribution
                    and logits by having logits represent likehood over evidence instead of posterior. Knowning the test
                    distribution they can obtain a balanced output posterior
                    by using the test distrubution prior and the network logits. They add a regulariser term to
                    explicitly disentangle logits
                    from source label distribution
                    and use known target label distribution to post compensate.
                </p>

                <p>
                    <strong>Problem</strong>
                    target distrubution may not be assumed uniform like many works do
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    frame LT as distribution shift, disentangle source
                    distribution and logits by having logits represent
                    likehood over evidence instead of posterior. add a
                    regulariser term to explicitly disentangle logits from source
                    label distribution and use known target label distribution to
                    post compensate

                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/lade-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/lade-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Distribution Alignment: A Unified Framework for Long-tail Visual Recognition
                </h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    zhang2021disalign, <br>
                    title={Distribution alignment: A unified framework for long-tail visual recognition}, <br>
                    author={Zhang, Songyang and Li, Zeming and Yan, Shipeng and He, Xuming and Sun, Jian}, <br>
                    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, <br>
                    year={2021}}
                </p>
                url=<a
                    href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Distribution_Alignment_A_Unified_Framework_for_Long-Tail_Visual_Recognition_CVPR_2021_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Distribution_Alignment_A_Unified_Framework_for_Long-Tail_Visual_Recognition_CVPR_2021_paper.pdf</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    This paper is tackling the problem of imbalance classification. They first start by setting up the
                    upper bound of the classification by including the test set in the
                    dataset, making the test set known and observed that the upper bound is not achieved by current
                    methods. They then propose a unified framework that learns a balanced
                    network in 2 stages. First they learn an adaptive calibration function that would linearly project
                    frozen classifier logits to a balanced space. To determine how much
                    of calibration to use on the logit, they use a linear function with 2 learnable parameters that
                    determine how much of the calibrated logits to use or the original logits
                    based on the input data sample. This method effectively achieve class and instance level calibration
                    of logits, as well as affine transformation of logits. They frame
                    the problem as a distribution shift between
                    training distribution (imbalanced) and target distribution (balanced), and minimize the KL
                    divergence between the 2 distributions, with test distribution being the
                    target distribution. The target distribution they conceived is a reweighted version of the training
                    distribution. The reweighting is done by using the inverse of the class frequency
                    so rare classes are weighted more than frequent classes.

                </p>

                <p>
                    <strong>Problem</strong>
                    imbalanced output logits
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    Learn affine logits adjustment at both class and instance levels.
                    Adjuste target posterior based on class weighting for logits to approximate.

                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/disalign-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/disalign-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Improving Calibration for Long-Tailed Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    zhong2021mislas, <br>
                    title={Improving calibration for long-tailed recognition}, <br>
                    author={Zhong, Zhisheng and Cui, Jiequan and Liu, Shu and Jia, Jiaya}, <br>
                    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, <br>
                    year={2021}}
                </p>

                url=<a
                    href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhong_Improving_Calibration_for_Long-Tailed_Recognition_CVPR_2021_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2021/papers/Zhong_Improving_Calibration_for_Long-Tailed_Recognition_CVPR_2021_paper.pdf</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    The authors are tackling the problem of imbalance classification. They observed that current methods
                    have poor calibration of their output
                    logits and are overconfident in their predictions. Usually by measuring the true accuracy against
                    the predicted accuracy, the true accuracy is
                    lower than the predicted accuracy. They also measure the ECE or expected calibration error and
                    observed that the ECE is high for current methods (lower being better).
                    First they propose to use Mixup to generate synthetic samples for the rare classes. Then they also
                    propose label smoothing of the classifier logits by
                    artificially deducting some epsilon value on the correct class logit and redistributing that value
                    to the other classes uniformally. Epsilon is
                    obtained as a function of the class sample count with some normalization by the difference between
                    the maximum and minimum class sample counts. To fit the
                    label aware smoothing objective, they combine cRT and LWS into classifier update function. It can
                    either scale the classifier logits (LWS) or apply a
                    learned update on the classifier weights (cRT) based on the retention factor r. For better
                    normalization on stage 2, they update the running mean and variance
                    in batch norm but fix the alpha and beta parameters in stage 2 (classifier update stage).

                </p>

                <p>
                    <strong>Problem</strong>
                    miss-calibration of output logits which leads to over-confidence in majority classes, where the true
                    accuracy is lower than the
                    predicted accuracy.
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    adjust target posterior as a function of class frequency.
                    use mixup to generate synthetic samples, and use a combination of LWS and cRT to calibrate the
                    output logits.

                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/mislas-fig1.PNG" class="img-fluid rounded" alt="...">

                    </div>
                    <div class="col">

                        <img src="../../images/imb_class/mislas-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">RSG: A Simple but Effective Module for Learning Imbalanced Datasets</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    wang2021rsg, <br>
                    title={Rsg: A simple but effective module for learning imbalanced datasets}, <br>
                    author={Wang, Jianfeng and Lukasiewicz, Thomas and Hu, Xiaolin and Cai, Jianfei and Xu, Zhenghua},
                    <br>
                    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, <br>
                    year={2021}}
                </p>

                url=<a
                    href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_RSG_A_Simple_but_Effective_Module_for_Learning_Imbalanced_Datasets_CVPR_2021_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_RSG_A_Simple_but_Effective_Module_for_Learning_Imbalanced_Datasets_CVPR_2021_paper.pdf</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    In this paper, the authors propose a simple but effective module called RSG to address the long-tail
                    problem. RSG works by
                    generating new samples for the rare classes. RSG uses variation information among the real samples
                    from the frequent classes to
                    generate new samples for the rare classes. RSG is composed of 3 modules:
                <ol>
                    <li>Center estimation module: to find class feature centers</li>
                    <li>Contrastive module: to check if 2 feature maps are from the same class</li>
                    <li>Vector Transformation module: generate new rare samples with displacement vectors</li>
                </ol>
                The center estimation module and the contrastive module are both trained using a a center estimation
                with sample contrastive loss
                and the vector transformation module is trained using a maximized vector loss. Injected between layers
                of a CNN, RSG takes the feature
                maps from the previous layer and outputs new synthetic samples for the rare classes. The center
                estimation module estimates takes the feature
                maps and estimate a set of centers in each clas which can be used a anchor for obtaining feature
                displacement of each sample. It is implemented
                as a linear model. The contrastive module ensures that no frequent-class-relevent information is present
                in the feature displacement. It is implemented
                with a CNN that will output a probability distribution of yes/no that 2 feature maps belong to the same
                class. The vector transformation module
                calculates the feature displacement of each frequent-class sample based on estimated centers and use the
                displacement to generate
                new samples for the rare classes. First a displacement vector is calculated for each sample by
                subtracting the sample's feature map from its closest
                upsampled center. Then the displacement vector is transformed by a convolutional layer to produce a
                displacement vector that will be applied the rare-class
                sample as opposed to the center to push away the decision boundary, thus enlarging the feature space.
                The transformed vector is optimized by a maximized vector loss
                to be collinear with the rare-class displacement vector to its center. the loss function is a linear
                combination of the center estimation with sample contrastive loss
                , the maximized vector loss and the classification loss.

                </p>
                <p>
                    <strong>Problem</strong>
                    Current methods on long-tail recognition lacks good generalization because they are not trained end
                    to end, and
                    variation information used to produce new synthetic data sample are not class-irrelevent.
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                <ol>
                    <li>Rare-class sample generator which can be trained end to end</li>
                    <li>Assuming class samples follow a multinomial distribution so there can be center(s) for each
                        class</li>
                    <li>Feature displacement indicates the displacement of a sample to its corresponding center in a
                        class and should not contain class-relevent information</li>
                    <li>Adding feature displacement vector to rare-class samples instead of centers to improve the
                        decision boundary</li>
                    <li>Transformed freq class displacement vector should be collinear to rare-class displacement vector
                        of the sample to be applied to</li>
                </ol>
                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/rsg-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/rsg-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Balanced Contrastive Learning for Long-Tailed Visual Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{
                    zhu2022bcl, <br>
                    title={Balanced contrastive learning for long-tailed visual recognition}, <br>
                    author={Zhu, Jianggang and Wang, Zheng and Chen, Jingjing and Chen, Yi-Ping Phoebe and Jiang,
                    Yu-Gang}, <br>
                    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, <br>
                    year={2022}}
                </p>

                url=<a
                    href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    In this paper, the authors investigate the geomtric structure formed by representation vectors of
                    classes and the class prototypes.
                    They observed that past methods fail to form regular simplex geometries in the feature space, which
                    is crucial for the generalization of the learned representations.
                    A regular simplex geometry is has 3 crucial characteristics
                <ol>
                    <li>the mean of all class prototypes should be the origin of the representation space</li>
                    <li>the class prototypes should be at a radius distance from the origin</li>
                    <li>the class prototypes should be vectors of which the dot product with one another can be
                        calculated</li>
                </ol>
                Most methods fail to satisfy the 1st characteristic, which means that the mean of all class prototypes
                is not the origin and therefore the class
                prototypes are not at equal distance from each other. To addres this the propose a 2 branch learning
                framework, where one branch is
                used to learn a feature extractor and a classifier called the classification branch. The other branch is
                used
                for contrastive learning called the contrastive learning branch. The classification branch employs
                cross-entropy loss and logit adjustment based on the
                class prior. The contrastive learning branch employs a novel loss called balanced contrastive loss (BCL)
                aiming to produce a regular simplex geometry in the feature space.
                The BCL loss is composed of 2 parts, a class averaging part and a class component part. During
                experimentation, the authors observed that
                when trained on imbalance data, regular Supervised Contrastive Learning (SCL), using the available
                expamples for each classes, grows the
                gradients for head classes far more than the tail classes. This is because the head classes have more
                examples than the tail classes. To alleviate that
                class averaging works by averaging the instances of each class in a minibach so that each class has the
                same approximate contribution to the optimization.
                The class component part, which carries the bulk of the positive results obtained, consists of
                introducing a class prototype for each class in all the minibatches.
                This allows all classes to be represented in all minibatches and therefore all classes contribute to the
                optimization stably. The class prototypes here are obtained
                from the MLP projection of the class specific weights of the linear classifier in the classification
                branch. The whole system is trained end-to-end.

                </p>
                <p>
                    <strong>Problem</strong>
                    Current approaches to long-tailed visual recognition (LTVR) fail to form regular simplex geometries
                    in the feature space, which is crucial for the generalization of the learned representations.
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                <ol>
                    <li>balanced feature space has a regular simplex geometry</li>
                    <li>class averaging reduces the effect head classes dominating the gradients in optimization</li>
                    <li>class component introduces class prototypes in all minibatches so that all classes contribute to
                        the optimization stably</li>
                    <li>logit adjustment for classifier logits using the imbalanced class priors</li>
                    <li>class prototypes are obtained from the MLP projection of the class specific weights of the
                        linear classifier in the classification branch</li>
                </ol>
                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/bcl-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/bcl-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Long-Tailed Recognition via Weight Balancing</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{alshammari2022weightbal, <br>
                    title={Long-tailed recognition via weight balancing}, <br>
                    author={Alshammari, Shaden and Wang, Yu-Xiong and Ramanan, Deva and Kong, Shu}, <br>
                    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, <br>
                    year={2022}} <br>
                </p>

                url=<a
                    href="https://openaccess.thecvf.com/content/CVPR2022/papers/Alshammari_Long-Tailed_Recognition_via_Weight_Balancing_CVPR_2022_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2022/papers/Alshammari_Long-Tailed_Recognition_via_Weight_Balancing_CVPR_2022_paper.pdf</a><br>

                <strong>Summary</strong>
                <br>
                <p>
                    This paper is tackling the problem of imbalance classification. They observed that imbalanced data cause the norm of the weights to be 
                    larger for the head classes than the tail classes. This is because the head classes have more examples than the tail classes. Ideally the 
                    weights should be balanced so that the norm of the weights are the same for all classes. To tackle this issue, they propose a 2 stage 
                    training framework. In the first stage, they train the classifier with a weight decay term that penalizes the norm of the weights. In the first
                    they use CE loss with a weight decay term. In the second stage, they train the classifier with weight decay and a maxnorm term that gives an upper
                    bound to the weight norms. The maxnorm term is obtained by using the lagrangian multiplier to convert the original constrained optimization problem
                    into an unconstrained optimization problem. The new object is a minmax objective when the original loss is minimized by the classifier weights and 
                    the maxnorm term is maximized by the KKT multiplier gamma. Intuitively, as gamma grows larger, the second term dominates the objective, forcing the 
                    learned weights to be less than the upper bound delta. to optimize both, the weights can be fixed to optimize gamma and then gamma can be fixed to
                    optimize the weights. Combined wih class balanced sampling, the method achieves very uniform weight norms. 

                </p>

                <p>
                    <strong>Problem</strong>
                    imbalanced learned weights in the classifier
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    apply weight decay in first stage as smaller weights are less imbalanced.
                    using lagrangian, update the loss with a maxnorm term thatgive an upper bound to the weight norms for second stage.
                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/weightbal-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/weightbal-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Nested Collaborative Learning for Long-Tailed Visual Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{li2022ncl, <br>
                    title={Nested collaborative learning for long-tailed visual recognition}, <br>
                    author={Li, Jun and Tan, Zichang and Wan, Jun and Lei, Zhen and Guo, Guodong}, <br>
                    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, <br>
                    year={2022}} <br>
                </p>
                url=<a
                    href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Nested_Collaborative_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Nested_Collaborative_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf</a><br>
                <strong>Summary</strong>
                <br>
                <p>
                    This paper tackle the issues of confident but incorrect predictions due to miscalibrated output logits by enlisting multiple experts. Each expert trains 
                    on a dataset with balanced softmax. In addition to the normal view of the dataset, they select a set of hard classes where a hard class is class that has 
                    the highest incorrect prediction. The hard classes training is also with balanced softmax and is nested within the normal training. Those hard classes allow 
                    to focus on learning feature that will help further discriminate them. The experts are trained
                    to have similar posterior distribution but not too much. This is done by minimizing the KL divergence between the posterior distributions of the experts.
                    However, if the expert are too similar, they are essential multiple instances of the same expert and the ensemble will not be diverse. To alleviate this,
                    the keep the lambda parameter in the KL divergence loss to be .6. Also by encourage the experts to have the same posterior they are, in adhoc manner, encourage 
                    the expert to distill their knowledge to each other. Finally they add a supervised contrastive loss term to improve the feature learning. 
                    The final prediction is obtained by aggregation of the experts output.
                </p>

                <p>
                    <strong>Problem</strong>
                    confident but incorrect predictions due to miscalibrated output logits
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                    training on hard classes nested within training on all classes. 
                    multiple experts trained to have similar posterior distribution but not too much


                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/ncl-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/ncl-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Targeted Supervised Contrastive Learning for Long-Tailed Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{li2022tsc, <br>
                    title={Targeted supervised contrastive learning for long-tailed recognition}, <br>
                    author={Li, Tianhong and Cao, Peng and Yuan, Yuan and Fan, Lijie and Yang, Yuzhe and Feris, Rogerio
                    S and Indyk, Piotr and Katabi, Dina}, <br>
                    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, <br>
                    year={2022}} <br>
                </p>
                url=<a
                    href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.pdf">
                    https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.pdf</a><br>
                <strong>Summary</strong>
                <br>
                <p>

                </p>

                <p>
                    <strong>Problem</strong>
                    supervised contrastive learning baseline suffer from poor uniformity brought in by imbalanced data
                    distribution. Poor uniformity
                    leads to poor separability in feature space.
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>
                <ol>
                    <li>generate uniform simplex geometry targets offline using stochastic gradient descent</li>
                    <li>assign each class to a target using hungarian algorithm</li>
                </ol>

                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/tsc-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/tsc-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Self-Supervised Aggregation of Diverse Experts for Test-Agnostic Long-Tailed
                    Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{zhang2022sade, <br>
                    title={Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition},
                    <br>
                    author={Zhang, Yifan and Hooi, Bryan and Hong, Lanqing and Feng, Jiashi}, <br>
                    journal={Advances in Neural Information Processing Systems}, <br>
                    year={2022}} <br>
                </p>
                url=<a
                    href="https://openreview.net/pdf?id=m7CmxlpHTiu">https://openreview.net/pdf?id=m7CmxlpHTiu</a><br>
                <strong>Summary</strong>
                <br>
                <p>

                </p>

                <p>
                    <strong>Problem</strong>
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>


                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/sade-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/sade-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Self Supervision to Distillation for Long-Tailed Visual Recognition</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{li2021ssd, <br>
                    title={Self supervision to distillation for long-tailed visual recognition}, <br>
                    author={Li, Tianhao and Wang, Limin and Wu, Gangshan}, <br>
                    booktitle={Proceedings of the IEEE/CVF international conference on computer vision}, <br>
                    year={2021}} <br>

                </p>
                url=<a
                    href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Self_Supervision_to_Distillation_for_Long-Tailed_Visual_Recognition_ICCV_2021_paper.pdf">
                    https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Self_Supervision_to_Distillation_for_Long-Tailed_Visual_Recognition_ICCV_2021_paper.pdf</a><br>

                <strong>Summary</strong>
                <br>
                <p>

                </p>

                <p>
                    <strong>Problem</strong>
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>


                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/ssd-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/ssd-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        <li>
            <div class="paper-container">
                <h3 class="paper-title">Decoupled Training for Long-Tailed Classification with Stochastic
                    Representations</h3>

                <p>
                    <strong>BibTex</strong>
                    <br>
                    @inproceedings{nam2023srepr, <br>
                    title={Decoupled Training for Long-Tailed Classification With Stochastic Representations}, <br>
                    author={Nam, Giung and Jang, Sunguk and Lee, Juho}, <br>
                    journal={The Eleventh International Conference on Learning Representations}, <br>
                    year={2023}} <br>

                </p>

                url=<a href="https://openreview.net/pdf?id=bcYZwYo-0t">https://openreview.net/pdf?id=bcYZwYo-0t</a><br>

                <strong>Summary</strong>
                <br>
                <p>

                </p>

                <p>
                    <strong>Problem</strong>
                </p>
                <p>
                    <strong>Solution, Ideas and Why</strong>


                </p>

                <strong>Images</strong><br>
                <div class="row">
                    <div class="col">
                        <img src="../../images/imb_class/srepr-fig1.PNG" class="img-fluid rounded" alt="...">
                    </div>
                    <div class="col">
                        <img src="../../images/imb_class/srepr-fig2.PNG" class="img-fluid rounded" alt="...">
                    </div>
                </div>
            </div>
        </li>
        </ol>
    </div>
    </div>
</body>

</html>